{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc6wG-x8VxWI"
   },
   "source": [
    "<a id='Top'></a>\n",
    "# CLASSIFICATION CHALLENGE\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Predict Overview: Climate Change Belief Analysis 2022\n",
    "\n",
    "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "With this context, EDSA is challenging you during the Classification Sprint with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYCbx3PSVxWK"
   },
   "source": [
    "\n",
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "## <a href=#one>1. Introduction</a>\n",
    "* <a href=#problemstament>1.1 Problem Statement</a>\n",
    "* <a href=#po>1.2 Project Objectives</a>\n",
    "* <a href=#dotrain_eda>1.3 Definition of Data Features</a>\n",
    "* <a href=#StartingaCometexperiment>1.4 Starting a Comet experiment</a>\n",
    "\n",
    "## <a href=#two>2. Import necessary libraries</a>\n",
    "\n",
    "<a href=#three>3. Loading Datasets</a>\n",
    "* <a href=#three1>3.1 Set Pandas to enable viewing of all columns</a>\n",
    "* <a href=#three2>3.2 Check the \"Shape\" of the data-sets</a>\n",
    "* <a href=#three21>3.3 Use the \".column\" function to view the columns in our data set</a>\n",
    "\n",
    "## <a href=#four>4. Data Preprocessing (Cleaning)</a>\n",
    "* <a href=#four1>4.1 Identifying Missing Values</a>\n",
    "* <a href=#four2>4.2 Removing Mentions</a>\n",
    "* <a href=#four3>4.3 Removing Hashtags</a>\n",
    "* <a href=#four4>4.4 Removing Accented characters</a>\n",
    "* <a href=#four5>4.5 Removing HTML tags</a>\n",
    "* <a href=#four6>4.6 Panding Contractions</a>\n",
    "* <a href=#four7>4.7 Removing URLs</a>\n",
    "* <a href=#four8>4.8 Removing Punctuations</a>\n",
    "* <a href=#four9>4.9 Removing Special Characters</a>\n",
    "* <a href=#four10>4.10 Tokenization</a>\n",
    "* <a href=#four11>4.11 Removing Retweets</a>\n",
    "* <a href=#four12>4.12 Conversion to Lower case</a>\n",
    "* <a href=#four13>4.13 Removing Stopwords</a>\n",
    "* <a href=#four14>4.14 Lemmatization</a>\n",
    "* <a href=#four15>4.15 Most Common Words</a>\n",
    "\n",
    "## <a href=#five>5. Exploratory Data Analysis (EDA)</a>\n",
    "* <a href=#five1>5.1 Explore the target variable</a>\n",
    "* <a href=#five2>5.2 Visualization</a>\n",
    "  * <a href=#five21>5.2.1 Checking the distribution of the data</a>\n",
    "  * <a href=#five22>5.2.2 Visualize tweet length for each class</a>\n",
    "  * <a href=#five23>5.2.3 Quick observations</a>\n",
    "* <a href=#five3>5.3 Investigate the feature variable</a>\n",
    "  * <a href=#five31>5.3.1 Sentiment Analysis</a>\n",
    "* <a href=#five4>5.3 Investigative text analysis</a>\n",
    "  * <a href=#five41>5.4.1 visualizing the text by sentiment</a>\n",
    "    * Anti Sentiment Class\n",
    "    * Neutral Sentiment Class\n",
    "    * Pro Sentiment Class\n",
    "    * News Sentiment Class\n",
    "  * <a href=#five42>5.4.2 Extract the top most used words</a>\n",
    "  * <a href=#five43>5.4.3 Quick observation</a>\n",
    "  * <a href=#five44>5.4.4 visualizing the data by sentiment</a>\n",
    "  * <a href=#five45>5.4.5 Quick observation</a>\n",
    "* <a href=#five5>5.5 Named Entity Recognition</a>\n",
    "  * <a href=#five51>5.5.1 Named Entity Recognition (Person)</a>\n",
    "    * Persons in Pro Sentiment Class\n",
    "    * Persons in Anti Sentiment Class\n",
    "    * Persons in Neutral Sentiment Class\n",
    "    * Persons in News Sentiment Class\n",
    "  * <a href=#five52>5.5.2 Named Entity Recognition (Organization)</a>\n",
    "    * Organizations in Pro Sentiment Class\n",
    "    * Organizations in News Sentiment Class\n",
    "    * Organizations in Neutral Sentiment Class\n",
    "    * Organizations in Anti Sentiment Class\n",
    "\n",
    "## <a href=#six>6. Classification Model</a>\n",
    "* <a href=#five1>6.1 Decision Tree Classifier</a>\n",
    "\n",
    "## <a href=#seven>7. Data or feature Engineering</a>\n",
    "* [7.1 `TF-IDF`](#tfidf)\n",
    "* [7.2 Pipelines](#Pipelines)\n",
    "* [7.2.1 Building classification pipelines](#Pipelines)\n",
    "* [7.3 Training models](#Pipelines)\n",
    "* [7.4 Model evalution](#Pipelines)\n",
    "* [7.4.1 Evalution of DecisionTreeClassifier](#Pipelines)\n",
    "* [7.4.2 Evalution of RandomForestClassifier](#Pipelines)\n",
    "* [7.4.3 Evaluation of LinearSVClassifier](#Pipelines)\n",
    "* [7.4.4 Evaluation of LGBMClassifier](#Pipelines)\n",
    "* [7.4.5 Evaluation of Logistic Regression](#Pipelines)\n",
    "* [7.4.6 Evaluation of SGD Classifier](#Pipelines)\n",
    "* [7.4.7 Support Vector Classfifier](#Pipelines)\n",
    "* [7.5 Model Comparision](#Pipelines)\n",
    "* [7.6 Ensemble Methods](#Pipelines)\n",
    "* [7.6.1 Method of Heterogeneous Ensembel](#Pipelines)\n",
    "* [7.6.2 Homogeneous Ensembles](#Pipelines)\n",
    "\n",
    "\n",
    "## <a href=#seven>7. Pipelines</a>\n",
    "  * [Building Classification Pipelines](#classifiers)\n",
    "  * [Using Word2Vec](#pipe_word2vec)\n",
    "\n",
    "## <a href=#seven>7. Modelling and Evaluation</a>\n",
    "  * [Modelling - Raw tweets](#raw_tweets)\n",
    "  * [Modelling - Cleaned tweets](#clean_tweets)\n",
    "  * [Model perfomance (raw tweets vs cleaned tweets)](#rvc)\n",
    "  * [Model perfomance (tfidf vs word2vec)](tfidfvsword2vec)\n",
    "\n",
    "  \n",
    "## <a href=#Eight>8. Ensemble Method</a>\n",
    "\n",
    "## <a href=#Nine>9. Selecting the best model</a>\n",
    "\n",
    "## <a href=#Ten>10. Predictions</a>\n",
    "\n",
    "## <a href=#Eleven>11. Submission</a>\n",
    "\n",
    "## <a href=#Twelve>12. conclusion</a>\n",
    "\n",
    "## <a href=#Thirteen>13. appendix</a>\n",
    "\n",
    "## <a href=#Fourteen>14. references</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLmvjzUHVxWL"
   },
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. INTRODUCTION\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: INTRODUCTION ⚡ |\n",
    "| :--------------------------- |\n",
    "| We will address the problem statement and objectives, as well as the classification of data aspects and a brief discussion of the Climate Change Belief Analysis in this part.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKSm-P-cVxWL"
   },
   "source": [
    "Companies are built with the goal of reducing one's environmental effect or carbon footprint. They provide ecologically friendly and sustainable goods and services that are in alignment with their principles and goals. They want to know how people feel about climate change and if they believe it is a serious threat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0TMPN2QVxWM"
   },
   "source": [
    " <a id=\"problemstament\"></a> \n",
    " \n",
    "### 1.1 Problem Statement.\n",
    "\n",
    "Create a Natural Language Processing model based on a particular twitter data to classify whether or not a person believes in climate change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0TMPN2QVxWM"
   },
   "source": [
    "<a id=\"po\"></a>\n",
    "\n",
    "### 1.2 Project Objectives\n",
    "\n",
    "* Clean the dataset so that it may be utilized for model development.\n",
    "\n",
    "* Create a variety of models to identify whether or not someone believes in climate change.\n",
    "\n",
    "* Using the provided Test Data, assess the model's accuracy in making predictions.\n",
    "\n",
    "* Pick the best model for determining whether or not someone believes in climate change.\n",
    "\n",
    "* Advise companies on the best strategy to determine if people believe in climate change and whether it is a serious concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0TMPN2QVxWM"
   },
   "source": [
    "<a id=\"dodf\"></a>\n",
    "\n",
    "### 1.3 Definition of Data Features\n",
    "\n",
    "Chris Bauch of the University of Waterloo received a Canada Foundation for Innovation JELF Grant to acquire these data. The collection includes tweets about climate change that were gathered between April 27, 2015 and February 21, 2018. There were 43943 tweets in total. Each tweet is assigned to one of the classes below:\n",
    "\n",
    "#### I. Class Description\n",
    "* 2 News: the tweet links to factual news about climate change\n",
    "* 1 Pro: the tweet supports the belief of man-made climate change\n",
    "* 0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "* -1 Anti: the tweet does not believe in man-made climate change\n",
    "\n",
    "#### II. Variable definitions\n",
    "- sentiment: Sentiment of tweet\n",
    "- message: Tweet body\n",
    "- tweetid: Twitter unique id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXS6GOF4VxWO"
   },
   "source": [
    "<a id=\"StartingaCometexperiment\"></a>\n",
    "\n",
    "### 1.4 Starting a Comet experiment\n",
    "<img src=\"https://www.comet.ml/images/logo_comet_light.png\" width=\"350\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "We will be using Comet as a form of version control throughout the development of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key=\"bEFY9Hn1QccermEDT6aTyQMOA\",\n",
    "    project_name=\"siders-analytics-zf5\",\n",
    "    workspace=\"kojosbk\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iv699tZoVxWa"
   },
   "source": [
    " <a id=\"two\"></a>\n",
    "## 2. Import necessary libraries\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Import necessary libraries ⚡ |\n",
    "| :--------------------------- |\n",
    "| We'd be importing all of the necessary libraries for the notebook to run smoothly..|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for data visualisation\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "from textblob import TextBlob\n",
    "from nltk.probability import FreqDist\n",
    "from wordcloud import WordCloud, ImageColorGenerator #Pip install wordcloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# imports for Natural Language  Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# imports model prosessing\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# imports Checking Acuracy\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "#Model evaluation packages\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import accuracy_score, precision_score,  recall_score\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from scikitplot.metrics import plot_roc, plot_confusion_matrix\n",
    "\n",
    "# imports for other libraires\n",
    "import pickle\n",
    "import unicodedata\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import en_core_web_sm\n",
    "import spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3wBf9yqVxWf"
   },
   "source": [
    "<a id=\"Three\"></a>\n",
    "## 3. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| The data from the `train` file is loaded into a DataFrame in this section.. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start importing our data, we make sure the raw data and notebook file are both in the same folder on our local system. Using the code below, we'll load the train data sets into our notebook. If the files aren't in the same folder, we'll have to point to the directory on our machine . To check that the data has loaded correctly, it is a good practice to call up the loaded data after it has been loaded.\n",
    "The information would be saved in two different data frames, one for training and testing and the other for our EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the train & test data sets\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test_with_no_labels.csv')\n",
    "\n",
    "# EDA Datasets\n",
    "train_eda = pd.read_csv('train.csv')\n",
    "test_eda = pd.read_csv('test_with_no_labels.csv')\n",
    "train_eda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=three1></a>\n",
    "\n",
    "#### 3.1 Set Pandas to enable viewing of all columns\n",
    "Due to the length of th content of the message column, pandas cannot display all of them at once by default. While doing EDA and data cleansing, we will need to see all of the columns. When the dataframe is presented, the code below allows us to see the whole set of columns in our data collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set option to display all columns\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=three2></a>\n",
    "\n",
    "#### 3.2 Check the \"Shape\" of the data-sets\n",
    "As demonstrated by the shape of both datasets, the data has been separated into two sets. The form also shows that the training data set has three columns, but the test data set has just two. Our model is designed to forecast the column that is not present in the test set. We can look for that specific item by looking for the missing entity (Column) in the test data set. After looking at both datasets, the column may be identified as the sentiment column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eda.shape, test_eda.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=three21></a>\n",
    "\n",
    "#### 3.3 Use the \".column\" function to view the columns in our data set\n",
    "The number of columns and rows in our data set were revealed by the shape method, but the.columns function provides the names of all the columns in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eda.columns, test_eda.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaXQWG2xVxWl"
   },
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Preprocessing (Cleaning)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "| ⚡ Description: Data Cleaning ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this phase, we'll convert the data into a readable and desired format, as well as filter out the most relevant information.. |\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0US091BsjPe"
   },
   "source": [
    "<a id=four1></a>\n",
    "\n",
    "#### 4.1 Identifying Missing Values\n",
    "\n",
    "It is important to identify the columns that have null entries as null values can affect the performance of our model. The \"isnull\" function shows the number of null values that are contained in each column of the dataset. This data set is relatively clean as this function shows that only the column \"Valencia_pressure\" features null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a copy of the dataframe\n",
    "train_data = train.copy()\n",
    "test_data = test.copy()\n",
    "\n",
    "#Cheching if there are missing values in the Train dataset\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cheching if there are missing values in the Test dataset\n",
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e-cdv2vxYDU"
   },
   "source": [
    "<a id=four2></a>\n",
    "\n",
    "#### 4.2 Removing Mentions\n",
    "\n",
    "After the @ symbol, the code extracts all characters (a-z for lower letters, A-Z for capital letters, 0-9 for any number, and an underscore because Twitter supports underscores in usernames). Then it will come to a halt if it encounters any character other than those listed in the brackets, such as a whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['message'] = train['message'].str.replace('@[A-Za-z0-9]+\\s?', '', regex=True)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=four3></a>\n",
    "\n",
    "#### 4.3 Removing Hashtags\n",
    "\n",
    "After the # sign, the code extracts all characters (a-z for lower letters, A-Z for capital letters, 0-9 for any number, and an underscore because Twitter supports underscores in usernames). Then it will come to a halt if it encounters any character other than those listed in the brackets, such as a whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['message'] = train['message'].str.replace('#[A-Za-z0-9]+\\s?', '', regex=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=four4></a>\n",
    "\n",
    "#### 4.4 Removing accented characters\n",
    " \n",
    "Accented characters/letters are common in text corpora, especially if you are simply interested in studying the English language. As a result, we must ensure that these characters are transformed to ASCII characters and standardized. Converting é to e is a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['message'] = train['message'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=four5></a>\n",
    "#### 4.5 Removing HTML tags\n",
    " \n",
    "Unstructured text frequently contains a lot of noise. HTML tags are a good example of a component that doesn't offer much to the interpretation and analysis of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['message'] = train['message'].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jELrgFWxiha"
   },
   "source": [
    "<a id=four6></a>\n",
    "#### 4.6 Panding Contractions\n",
    " \n",
    "Words or syllables are abbreviated in contractions. In the English language, they are frequently found in both written and spoken form. By deleting key letters and sounds, these shorter versions or contractions of words are generated. In the case of English contractions, one of the vowels is frequently removed from the word. \"Do not\" becomes \"don't,\" while \"I would\" becomes \"I'd.\" Text standardization is aided by converting each contraction to its enlarged, original form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['message'] = train['message'].apply(decontracted)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqX-qoppGinx"
   },
   "source": [
    "<a id=four7></a>\n",
    "\n",
    "#### 4.7 Removing URLs\n",
    "\n",
    "There is often a lot of noise in unstructured text. URL tags are an example of a component that doesn't provide much in the way of content interpretation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['message'] = train['message'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "train['message'] = train['message'].str.replace(r's*https?://S+(s+|$)', ' ',case=False).str.strip()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aStx7k71xteX"
   },
   "source": [
    "<a id=four8></a>\n",
    "\n",
    "#### 4.8 Removing Punctuations\n",
    "\n",
    "In textual data, there is generally a lot of noise. Punctuations are an example of a component that offers nothing in the way of textual analysis and interpretation. Performing this operation aids in text uniformity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(x):\n",
    "  '''\n",
    "  This function takes a string and removes all punctuations from it. and sends it back\n",
    "  '''\n",
    "  x = re.sub(r\"([^A-Za-z0-9]+)\", \" \", x)\n",
    "  return x\n",
    "\n",
    "\n",
    "train['message'] = train['message'].map(lambda x: remove_punc(str(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=four9></a>\n",
    "\n",
    "#### 4.9 Removing Special Characters and numbers\n",
    " \n",
    "Non-alphanumeric characters or, on rare occasions, numeric characters are used as special characters and symbols in unstructured text, adding to the added noise. Simple regular expressions (regexes) may usually be utilized to get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['message'] = train['message'].str.replace('\\d+', '')\n",
    "train['message'] = train['message'].apply(remove_special_characters)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aEWBGgcx6U2"
   },
   "source": [
    "<a id=four10></a>\n",
    "\n",
    "#### 4.10 Tokenization\n",
    "A tokeniser breaks text into a series of tokens that are roughly equivalent to \"words.\" Tokenisers will be used to clean up the data and make it ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "train['message'] = train['message'].apply(word_tokenize)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95GYlV-xx_Ld"
   },
   "source": [
    "<a id=four11></a>\n",
    "\n",
    "#### 4.11 Removing Retweets\n",
    "\n",
    "We remove some more noise by deleting the \"RT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet = 'RT'\n",
    "train['message'] = train['message'].apply(lambda x: [word for word in x if word not in retweet])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iu74caNEyUdF"
   },
   "source": [
    "<a id=four12></a>\n",
    "\n",
    "#### 4.12 Conversion to Lower case\n",
    "\n",
    "we make all the text lower case to remove some noise from capitalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['message'] = train['message'].apply(lambda x: [word.lower() for word in x])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qKaj-1U-MeY"
   },
   "source": [
    "<a id=four13></a>\n",
    "\n",
    "#### 3.13 Removing Stopwords\n",
    "\n",
    "Stop words are terms that should not be used in Search Queries since they have no significant meaning. These terms are usually kept out of search searches since they return a lot of irrelevant data. Stopwords are found in the corpus of `nltk`. Let's make a list of English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following lamda function removes all of the English stopwords from the input text.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "train['message'] = train['message'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peHsnPqZ-Z-Z"
   },
   "source": [
    "<a id=four14></a>\n",
    "\n",
    "#### 4.14 Lemmatization\n",
    "\n",
    "Lemmatization is a process that is quite similar to stemming. The process of lemmatizing is the grouping of words with related meanings. So, while you can't look up your root stem (the word you end up with) in a dictionary, you can look up a lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "train['pos_tags'] = train['message'].apply(nltk.tag.pos_tag)\n",
    "train['wordnet_pos'] = train['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
    "\n",
    "train['message'] = train['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
    "train = train.drop(['wordnet_pos','pos_tags'],axis =1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7idoQ_Dw-oZy"
   },
   "source": [
    "<a id=four15></a>\n",
    "\n",
    "#### 4.15 Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for message in train['message'].values:\n",
    "    for word in message:\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eceDA71t-z6b"
   },
   "source": [
    "Separate Datframes of Tweets for each Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VqeCRDaVxXY"
   },
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| We'll use a range of strategies in this part to maximize specific insights into our dataset, uncover underlying structure, extract relevant variables, find outliers and anomalies, test assumptions, and establish the optimum estimation parameters. In other words, we want to go deeper into our dataset in order to learn more about its behavior! |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Separate Datframes of Tweets for each Sentiment *\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"See distribution of messages per sentiment : \")\n",
    "count = train.groupby(\"sentiment\").count()[\"message\"].reset_index().sort_values(by=\"message\", ascending=False)\n",
    "count.style.background_gradient(cmap=\"Purples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five1></a>\n",
    "\n",
    "#### 5.1 Explore the target variable\n",
    "\n",
    "For each of our categories, we want to know how our target variable behaves....let's dig in ⛏!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the target variable name to their code for better understanding\n",
    "\n",
    "train_eda['Sentiment_Labels']  = train_eda['sentiment'].map({-1: 'Anti',0:'Neutral', 1:'Pro', 2:'News'})\n",
    "\n",
    "#Confirm the dataset\n",
    "train_eda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five2></a>\n",
    "\n",
    "### 5.2 Visualization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five21></a>\n",
    "\n",
    "#### 5.2.1 Checking the distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Funnel(\n",
    "    y = [\"Pro\",\"News\", \"Nuetral\", \"Anti\"],\n",
    "    x = round(train.sentiment.value_counts(normalize = True) * 100,2)\n",
    "    ))\n",
    "fig.update_layout(height=700,width = 900, showlegend=False, title_text=\"Tweets data distribution by sentiment\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display target distribution\n",
    "\n",
    "print(\"Percentage contribution:\\n\",train_eda.Sentiment_Labels.value_counts(normalize=True)*100)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, \n",
    "                         nrows=1, \n",
    "                         figsize=(20, 15), \n",
    "                         dpi=100)\n",
    "\n",
    "sns.countplot(train_eda['Sentiment_Labels'], ax=axes[0])\n",
    "\n",
    "code_labels=['Pro', 'News', 'Neutral', 'Anti']\n",
    "axes[1].pie(train_eda['Sentiment_Labels'].value_counts(),\n",
    "            labels= code_labels,\n",
    "            autopct='%1.0f%%',\n",
    "            startangle=90,\n",
    "            explode = (0.1, 0.1, 0.1, 0.1))\n",
    "\n",
    "fig.suptitle('Dispersion of the target variable', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of each message for each class\n",
    "train_eda['text_length'] = train_eda['message'].apply(lambda x: len(x))\n",
    "train_eda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five22></a>\n",
    "\n",
    "#### 5.2.2 Visualize tweet length for each class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=train_eda['Sentiment_Labels'], y=train_eda['text_length'], data=train_eda, width = 0.9, color = 'indigo')\n",
    "plt.ylabel('Length of the message')\n",
    "plt.xlabel('Sentiment class')\n",
    "plt.title('Message length for each sentiment class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the text length for each class type\n",
    "sns.violinplot(x=\"Sentiment_Labels\", y=\"text_length\", data=train_eda)\n",
    "plt.ylabel('Lenght of the message')\n",
    "plt.xlabel('Sentiment class')\n",
    "plt.title('Message length for each sentiment class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five23></a>\n",
    "\n",
    "##### 5.2.3 Quick observations:\n",
    "* Our target data, including the feature message, has no missing values.\n",
    "\n",
    "* There is a significant disparity in sentiment groups.\n",
    "\n",
    "* Sentiment class '1' (Pro) leads the graph with almost 54% of the vote, while class '-1' (Anti) comes in second with 8%.\n",
    "\n",
    "* Both the upper and lower bounds of Sentiment Class Pro have outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five3></a>\n",
    "\n",
    "### 5.3 Investigate the feature variable\n",
    "\n",
    "We want to obtain a better understanding of the message and what it includes at this point in order to draw useful conclusions about our target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five31></a>\n",
    "\n",
    "#### 5.3.1 Sentiment Analysis\n",
    "\n",
    "\n",
    "The process of sentiment analysis finds the underlying emotions in a piece of text. It is the process of categorizing literature into good, negative, or neutral categories. The values range from 1 to -1, with positive emotions decreasing from 1 to -1 and neutral emotion remaining at 0.\n",
    "\n",
    "---\n",
    "![Alt_text](https://editor.analyticsvidhya.com/uploads/61727sentiment-fig-1-689.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the sentiment analysis from raw data\n",
    "\n",
    "\n",
    "train_eda['polarity']=train_eda['message'].apply(lambda x:\n",
    "                                             TextBlob(x).sentiment.polarity)\n",
    "\n",
    "plt.figure(figsize=[10, 8])\n",
    "plt.hist(train_eda['polarity'], bins=25, linewidth=0)\n",
    "plt.gca().set(title='Message polarity of raw data',\n",
    "              ylabel='Frequency', xlabel = 'Sentiment value')\n",
    "plt.axvline(train_eda['polarity'].mean(), color='black',\n",
    "            linestyle='dashed', linewidth=1)\n",
    "plt.axvline(train_eda['polarity'].median(), color='orange',\n",
    "            linestyle='dashed', linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the sentiment analysis from cleaned data\n",
    "\n",
    "# Convert the cleaned message from list to string\n",
    "train['message'] = train['message'].str.join(' ')\n",
    "\n",
    "train['polarity']=train['message'].apply(lambda x:\n",
    "                                             TextBlob(x).sentiment.polarity)\n",
    "\n",
    "plt.figure(figsize=[10, 8])\n",
    "plt.hist(train['polarity'], bins=25, linewidth=0)\n",
    "plt.gca().set(title='Message polarity of cleaned data',\n",
    "              ylabel='Frequency', xlabel = 'Sentiment value')\n",
    "plt.axvline(train['polarity'].mean(), color='black',\n",
    "            linestyle='dashed', linewidth=1)\n",
    "plt.axvline(train['polarity'].median(), color='orange',\n",
    "            linestyle='dashed', linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five32></a>\n",
    "#### 5.3.2 Observations\n",
    "\n",
    "- Positively biased feelings, with the mean being higher than the median.\n",
    "- The majority of tweets are of a neutral nature.\n",
    "- Although cleaned data has a higher mean than raw data, the central tendency for both shows a similar trend.\n",
    "- This guarantees that we didn't dilute the behavior of our data during the cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five4></a>\n",
    "\n",
    "#### 5.4 Investigative  text analysis\n",
    "From here, we'll delve further into our text, looking into each term and its frequency of occurrence individually.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud for cleaned data\n",
    "train['message'] = train['message'].map(lambda x: list(map(str, filter(None, x.split(' '))))) ## Convert the cleaned message from string to list\n",
    "words = train['message']\n",
    "allwords = []\n",
    "for wordlist in words:\n",
    "    allwords += wordlist\n",
    "    \n",
    "mostcommon = FreqDist(allwords).most_common(10000)\n",
    "wordcloud = WordCloud(width=700, height=700, background_color=\n",
    "                      'black').generate(str(mostcommon))\n",
    "fig = plt.figure(figsize=(30,10), facecolor='white')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five41></a>\n",
    "\n",
    "#### 5.4.1 visualizing the text by sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to strings for each sentiment class\n",
    "news = train['message'][train_eda['sentiment']==2].str.join(' ')\n",
    "neutral = train['message'][train_eda['sentiment']==2].str.join(' ')\n",
    "pro = train['message'][train_eda['sentiment']==2].str.join(' ')\n",
    "anti = train['message'][train_eda['sentiment']==2].str.join(' ')\n",
    "\n",
    "#Visualize each sentiment class\n",
    "\n",
    "fig, axis = plt.subplots(nrows=2, ncols=2, figsize=(18, 12))\n",
    "\n",
    "news_wordcloud = WordCloud(width=900, height=600, background_color='black', colormap='summer').generate(str(news))\n",
    "axis[0, 0].imshow(news_wordcloud)\n",
    "axis[0, 0].set_title('News Class',fontsize=14)\n",
    "axis[0, 0].axis(\"off\") \n",
    "\n",
    "neutral_wordcloud = WordCloud(width=900, height=600, background_color='black', colormap='winter', min_font_size=10).generate(str(neutral))\n",
    "axis[1, 0].imshow(neutral_wordcloud)\n",
    "axis[1, 0].set_title('Neutral Class',fontsize=14)\n",
    "axis[1, 0].axis(\"off\") \n",
    "\n",
    "pro_wordcloud = WordCloud(width=900, height=600, background_color='black', colormap='icefire', min_font_size=10).generate(str(pro))\n",
    "axis[0, 1].imshow(pro_wordcloud)\n",
    "axis[0, 1].set_title('Pro Class',fontsize=14)\n",
    "axis[0, 1].axis(\"off\") \n",
    "\n",
    "anti_wordcloud = WordCloud(width=900, height=600, background_color='black', colormap='winter', min_font_size=10).generate(str(anti))\n",
    "axis[1, 1].imshow(anti_wordcloud)\n",
    "axis[1, 1].set_title('Anti Class',fontsize=14)\n",
    "axis[1, 1].axis(\"off\") \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before completing the exploratory data analysis\n",
    "\n",
    "Let's have a look at how the sentiment class reacts to climate change through tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = train_eda.groupby('sentiment')\n",
    "Anti = \"\".join(gb.get_group(-1)['message'])\n",
    "Neutral = \"\".join(gb.get_group(0)['message'])\n",
    "Pro = \"\".join(gb.get_group(1)['message'])\n",
    "News = \"\".join(gb.get_group(2)['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Anti Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Show the full_text of 5 Anti Sentiment Class tweets: ')\n",
    "for tweet in gb.get_group(-1)['message'][100:105]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a result of the above, the Anti Sentiment class thinks that global climate change is a government-engineered hoax.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Neutral Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Show the full_text of 5 Anti Sentiment Class tweets: ')\n",
    "for tweet in gb.get_group(0)['message'][1000:1005]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The class of neutral feelings is always neutral; they are neither proponents nor opponents of the subject at hand.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Pro Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Show the full_text of 5 Anti Sentiment Class tweets: ')\n",
    "for tweet in gb.get_group(1)['message'][100:105]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Climate Change Activists make up the majority of the Pro Sentiment class, and they have a no-holds-barred attitude toward government engagement in climate change.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## News Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Show the full_text of 5 Anti Sentiment Class tweets: ')\n",
    "for tweet in gb.get_group(2)['message'][100:105]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The news sentiment class is generally curious, and they want to know the truth about climate change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five42></a>\n",
    "\n",
    "#### 5.4.2 Extracting the top  most used words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Extract the hastags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hashtags from raw data\n",
    "\n",
    "train_eda = pd.read_csv('train.csv')\n",
    "train_eda['no_stopwords'] = train_eda['message'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "hashtag = [item for item in train_eda['no_stopwords'].str.split() if item[0].startswith('#')]\n",
    "hashtag = sum(hashtag, [])\n",
    "count = nltk.FreqDist(hashtag)  \n",
    "hashtag = pd.DataFrame({'hashtags': list(count.keys()),\n",
    "                       'count': list(count.values())})\n",
    "hashtag = hashtag.nlargest(20, columns=\"count\")\n",
    "hashtag['hashtags'] = hashtag['hashtags'].map(lambda x: remove_punc(str(x))).str.lower().str.strip()\n",
    "hashtag = hashtag.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "hashtag = (hashtag.groupby(\"hashtags\").agg({\"count\": np.sum}))\n",
    "hashtag = hashtag.sort_values(by='count', ascending=True)\n",
    "hashtag = hashtag.drop(index=\"\")\n",
    "hashtag.index = hashtag.index.str.capitalize()\n",
    "\n",
    "hashtag.sort_values(by='count', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(hashtag, x='count', y=hashtag.index, color='count', title='Top 15 Hashtags in the whole data',\n",
    "             template='plotly_white', labels={'ngram': 'Bigram', 'count': 'Count'}).update_xaxes(categoryorder='total ascending')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Visualizing in a tree map view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.treemap(hashtag, title='Treemap chart by Top 15 Hashtags in the whole data',\n",
    "                 path=[hashtag.index], values = 'count',color='count', color_continuous_scale=px.colors.sequential.GnBu)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five43></a>\n",
    "\n",
    "##### 5.4.3 Quick observation\n",
    "- Climate, Change, Global, and warming are the top four hashtags, which is to be expected given that these are the important phrases in ensuring that tweets are easily found\n",
    "- According to  [this article](https://www.azocleantech.com/article.aspx?ArticleID=3), there is a link between climate change and global warming, which explains why the hashtags \"global\" and \"warming\" are in the top four.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five44></a>\n",
    "\n",
    "#### 5.4.4 visualizing text data by sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start-off by sorting the dataframe for each sentiment class\n",
    "\n",
    "# News\n",
    "news_hashtag = [item for item in train_eda['no_stopwords'][train_eda['sentiment'] == 2].str.split() if item[0].startswith('#')]\n",
    "news_hashtag = sum(news_hashtag, [])\n",
    "count = nltk.FreqDist(news_hashtag)  \n",
    "news_hashtag = pd.DataFrame({'news_hashtags': list(count.keys()),\n",
    "                       'count': list(count.values())})\n",
    "news_hashtags = news_hashtag.nlargest(10, columns=\"count\")\n",
    "news_hashtags = news_hashtags.drop(index=17)\n",
    "news_hashtags['news_hashtags'] = news_hashtags['news_hashtags'].map(lambda x: remove_punc(str(x))).str.strip()\n",
    "news_hashtags['news_hashtags'] = news_hashtags['news_hashtags'].str.capitalize()\n",
    "news_hashtags = news_hashtags.sort_values(by='count', ascending=False)\n",
    "\n",
    "# Pro\n",
    "pro_hashtag = [item for item in train_eda['no_stopwords'][train_eda['sentiment'] == 1].str.split() if item[0].startswith('#')]\n",
    "pro_hashtag = sum(pro_hashtag, [])\n",
    "count = nltk.FreqDist(pro_hashtag)  \n",
    "pro_hashtag = pd.DataFrame({'pro_hashtags': list(count.keys()),\n",
    "                       'count': list(count.values())})\n",
    "pro_hashtags = pro_hashtag.nlargest(10, columns=\"count\")\n",
    "pro_hashtags['pro_hashtags'] = pro_hashtags['pro_hashtags'].map(lambda x: remove_punc(str(x))).str.lower().str.strip()\n",
    "pro_hashtags = (pro_hashtags.groupby(\"pro_hashtags\").agg({\"count\": np.sum}))\n",
    "pro_hashtags.index = pro_hashtags.index.str.capitalize()\n",
    "pro_hashtags = pro_hashtags.sort_values(by='count', ascending=True)\n",
    "\n",
    "# Neutral\n",
    "neut_hashtag = [item for item in train_eda['no_stopwords'][train_eda['sentiment'] == 0].str.split() if item[0].startswith('#')]\n",
    "neut_hashtag = sum(neut_hashtag, [])\n",
    "count = nltk.FreqDist(neut_hashtag)  \n",
    "neut_hashtag = pd.DataFrame({'neutral_hashtags': list(count.keys()),\n",
    "                       'count': list(count.values())})\n",
    "neutral_hashtags = neut_hashtag.nlargest(10, columns=\"count\")\n",
    "neutral_hashtags = neutral_hashtags.drop(index=4)\n",
    "neutral_hashtags['neutral_hashtags'] = neutral_hashtags['neutral_hashtags'].map(lambda x: remove_punc(str(x))).str.lower().str.strip()\n",
    "neutral_hashtags = (neutral_hashtags.groupby(\"neutral_hashtags\").agg({\"count\": np.sum}))\n",
    "neutral_hashtags.index = neutral_hashtags.index.str.capitalize()\n",
    "neutral_hashtags = neutral_hashtags.sort_values(by='count', ascending=False)\n",
    "\n",
    "# Anti\n",
    "anti_hashtag = [item for item in train_eda['no_stopwords'][train_eda['sentiment'] == -1].str.split() if item[0].startswith('#')]\n",
    "anti_hashtag = sum(anti_hashtag, [])\n",
    "count = nltk.FreqDist(anti_hashtag)  \n",
    "anti_hashtag = pd.DataFrame({'anti_hashtags': list(count.keys()),\n",
    "                       'count': list(count.values())})\n",
    "anti_hashtags = anti_hashtag.nlargest(10, columns=\"count\")\n",
    "anti_hashtags['anti_hashtags'] = anti_hashtags['anti_hashtags'].map(lambda x: remove_punc(str(x))).str.strip()\n",
    "anti_hashtags['anti_hashtags'] = anti_hashtags['anti_hashtags'].str.capitalize()\n",
    "anti_hashtags = anti_hashtags.sort_values(by='count', ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Ploting the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, start_cell=\"bottom-left\")\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    specs=[[{}, {}],\n",
    "           [{}, {}]],\n",
    "    subplot_titles=(\"Anti Hashtags\",\"Neutral Hashtags\", \"Pro Hashtags\", \"News_hashtags\"))\n",
    "\n",
    "\n",
    "fig.add_trace(go.Bar(y=anti_hashtags['count'], x= anti_hashtags[\"anti_hashtags\"]),\n",
    "              row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(x=neutral_hashtags.index, y=neutral_hashtags['count']),\n",
    "              row=1, col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(x=pro_hashtags.index, y=pro_hashtags[\"count\"]),\n",
    "              row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(x=news_hashtags[\"news_hashtags\"], y=news_hashtags[\"count\"]),\n",
    "              row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=700,width = 900, showlegend=False, title_text=\"Top hashtags comparison for different sentiment classes\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five45></a>\n",
    "\n",
    "##### 5.4.5 Quick observation\n",
    "- Climate, Change, Global, and warming are the top four hashtags, which is to be expected given that these are the important phrases in ensuring that tweets are easily found\n",
    "- According to  [this article](https://www.azocleantech.com/article.aspx?ArticleID=3), there is a link between climate change and global warming, which explains why the hashtags \"global\" and \"warming\" are in the top four.\n",
    "- The anti hastag group uses the word \"faked\" as one of their top four most often used words to demonstrate that they do not believe in global warming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five5></a>\n",
    "\n",
    "### 5.5 Named Entity Recognition\n",
    "- in the section we would determine the names of the persons and organizations who are most frequently mentioned in each sentiment category.\n",
    "\n",
    "(Named Entity Recognition (NER) is a technique that parses a sentence or a block of text for entities that may be classified as names, organizations, locations, quantities, monetary values, percentages, and so on. Only names, locations, and organizations were incorporated in traditional NER algorithms. They may now be dynamically taught to extract more than only the entities listed earlier. If a chunk of entities is located, NER is a simple but effective method )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=five51></a>\n",
    "#### 5.5.1 Named Entity Recognition (Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sent_labels']  = train['sentiment'].map({-1: 'Anti',0:'Neutral', 1:'Pro', 2:'News'})\n",
    "train.head(3)\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.max_length = 4000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = train[train['sent_labels'].isin(['Anti','Neutral','Pro','News'])]\n",
    "tokens = nlp(''.join(str(train_a['message'].tolist())))\n",
    "items = [word.text for word in tokens.ents]\n",
    "\n",
    "person_list = []\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ =='PERSON':\n",
    "         person_list.append(ent.text)\n",
    "\n",
    "person_counts = Counter(person_list).most_common()\n",
    "\n",
    "df_a = pd.DataFrame(person_counts, columns=['person','count'])\n",
    "df_a['person'] = df_a['person'].str.capitalize()\n",
    "\n",
    "df_a.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.treemap(df_a, path=['person'],title='Treemap chart by Top Persons in the whole data',\n",
    "                 values='count', color= \"count\")\n",
    " \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The class of neutral feelings is always neutral; they are neither proponents nor opponents of the subject at hand.\n",
    "\n",
    "---\n",
    "* ### Persons in Pro Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PEOPLE INCLUDED IN PRO SENTIMENT\n",
    "train_pro = train[train['sent_labels'] == 'Anti']\n",
    "tokens = nlp(''.join(str(train_pro['message'].tolist())))\n",
    "items = [word.text for word in tokens.ents]\n",
    "\n",
    "person_list = []\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ =='PERSON':\n",
    "         person_list.append(ent.text)\n",
    "\n",
    "person_counts = Counter(person_list).most_common()\n",
    "\n",
    "df_pro = pd.DataFrame(person_counts, columns=['person','count'])\n",
    "df_pro['person'] = df_pro['person'].str.capitalize()\n",
    "\n",
    "df_pro.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.treemap(df_pro, path=['person'],title='Treemap chart by Top Persons in the Pro Sentiment Section',\n",
    "                 values='count', color= \"count\")\n",
    " \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The class of neutral feelings is always neutral; they are neither proponents nor opponents of the subject at hand.\n",
    "\n",
    "---\n",
    "* ### Persons in Anti Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PEOPLE INCLUDED IN ANTI SENTIMENT\n",
    "train_anti = train[train['sent_labels'] == 'Anti']\n",
    "tokens = nlp(''.join(str(train_anti['message'].tolist())))\n",
    "items = [word.text for word in tokens.ents]\n",
    "\n",
    "person_list = []\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ =='PERSON':\n",
    "         person_list.append(ent.text)\n",
    "\n",
    "person_counts = Counter(person_list).most_common()\n",
    "\n",
    "df_anti = pd.DataFrame(person_counts, columns=['person','count'])\n",
    "df_anti['person'] = df_anti['person'].str.capitalize()\n",
    "\n",
    "df_anti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.treemap(df_anti, path=['person'],title='Treemap chart by Top Persons in the Anti Sentiment Section',\n",
    "                 values='count', color= \"count\")\n",
    " \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The class of neutral feelings is always neutral; they are neither proponents nor opponents of the subject at hand.\n",
    "\n",
    "---\n",
    "* ### Persons in Neutral Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PEOPLE INCLUDED IN NEUTRAL SENTIMENT\n",
    "train_neutral = train[train['sent_labels'] == 'Neutral']\n",
    "tokens = nlp(''.join(str(train_neutral['message'].tolist())))\n",
    "items = [word.text for word in tokens.ents]\n",
    "\n",
    "person_list = []\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ =='PERSON':\n",
    "         person_list.append(ent.text)\n",
    "\n",
    "person_counts = Counter(person_list).most_common()\n",
    "\n",
    "df_neutral = pd.DataFrame(person_counts, columns=['person','count'])\n",
    "df_neutral['person'] = df_neutral['person'].str.capitalize()\n",
    "\n",
    "df_neutral.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.treemap(df_neutral, path=['person'],title='Treemap chart by Top Persons in the neutral Sentiment Section',\n",
    "                 values='count', color= \"count\")\n",
    " \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The class of neutral feelings is always neutral; they are neither proponents nor opponents of the subject at hand.\n",
    "\n",
    "---\n",
    "* ### Persons in News Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PEOPLE INCLUDED IN NEWS SENTIMENT\n",
    "train_news = train[train['sent_labels'] == 'News']\n",
    "tokens = nlp(''.join(str(train_news['message'].tolist())))\n",
    "items = [word.text for word in tokens.ents]\n",
    "\n",
    "person_list = []\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ =='PERSON':\n",
    "         person_list.append(ent.text)\n",
    "\n",
    "person_counts = Counter(person_list).most_common()\n",
    "\n",
    "df_news = pd.DataFrame(person_counts, columns=['person','count'])\n",
    "df_news['person'] = df_news['person'].str.capitalize()\n",
    "\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.treemap(df_pro, path=['person'],title='Treemap chart by Top Persons in the Pro Sentiment Section',\n",
    "                 values='count', color= \"count\")\n",
    " \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6.2 observations\n",
    "*\n",
    "*\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6.3 Named Entity Recognition (ORGANIZATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORGANIZATIONS INCLUDED IN Pro SENTIMENT\n",
    "train_oa = train[train['sent_labels'].isin(['Anti','Neutral','Pro','News'])]\n",
    "tokens = nlp(''.join(str(train_oa['message'].tolist())))\n",
    "items = [word.text for word in tokens.ents]\n",
    "\n",
    "org_list = []\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ =='ORG':\n",
    "        org_list.append(ent.text)\n",
    "\n",
    "org_counts = Counter(org_list).most_common()\n",
    "\n",
    "df_org_a = pd.DataFrame(org_counts, columns=['organization','count'])\n",
    "df_org_a['organization'] = df_org_a['organization'].str.upper()\n",
    "\n",
    "df_org_a.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the data\n",
    "fig = px.treemap(df_org_a, path=['organization'],title='Treemap chart by Top organizations in the whole data',\n",
    "                 values='count', color= \"count\")\n",
    " \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The class of neutral feelings is always neutral; they are neither proponents nor opponents of the subject at hand.\n",
    "\n",
    "---\n",
    "* ### Organizations in Pro Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORGANIZATIONS INCLUDED IN Pro SENTIMENT\n",
    "train_pro = train[train['sent_labels'] == 'Pro']\n",
    "tokens = nlp(''.join(str(train_pro['message'].tolist())))\n",
    "items = [word.text for word in tokens.ents]\n",
    "\n",
    "org_list = []\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ =='ORG':\n",
    "        org_list.append(ent.text)\n",
    "\n",
    "org_counts = Counter(org_list).most_common()\n",
    "\n",
    "df_org_pro = pd.DataFrame(org_counts, columns=['organization','count'])\n",
    "df_org_pro['organization'] = df_org_pro['organization'].str.upper()\n",
    "\n",
    "df_org_pro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df_org_pro, values='count', names='organization',\n",
    "             title='Pie chart by Top organizations in the Pro Sentiment Section',)\n",
    "fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The class of neutral feelings is always neutral; they are neither proponents nor opponents of the subject at hand.\n",
    "\n",
    "---\n",
    "* ### Organizations in News Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORGANIZATIONS INCLUDED IN NEWS SENTIMENT\n",
    "train_news = train[train['sent_labels'] == 'News']\n",
    "tokens = nlp(''.join(str(train_news['message'].tolist())))\n",
    "items = [word.text for word in tokens.ents]\n",
    "\n",
    "org_list = []\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ =='ORG':\n",
    "        org_list.append(ent.text)\n",
    "\n",
    "org_counts = Counter(org_list).most_common()\n",
    "\n",
    "df_org = pd.DataFrame(org_counts, columns=['organization','count'])\n",
    "df_org['organization'] = df_org['organization'].str.upper()\n",
    "\n",
    "df_org.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df_org, values='count', names='organization',\n",
    "             title='Pie chart by Top organizations in the News Sentiment Section',)\n",
    "fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The class of neutral feelings is always neutral; they are neither proponents nor opponents of the subject at hand.\n",
    "\n",
    "---\n",
    "* ### Organizations in Neutral Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORGANIZATIONS INCLUDED IN NEUTRAL SENTIMENT\n",
    "train_neutral = train[train['sent_labels'] == 'Neutral']\n",
    "tokens = nlp(''.join(str(train_neutral['message'].tolist())))\n",
    "items = [word.text for word in tokens.ents]\n",
    "\n",
    "org_list = []\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ =='ORG':\n",
    "        org_list.append(ent.text)\n",
    "\n",
    "org_counts = Counter(org_list).most_common()\n",
    "\n",
    "df_org_neutral= pd.DataFrame(org_counts, columns=['organization','count'])\n",
    "df_org_neutral['organization'] = df_org_neutral['organization'].str.upper()\n",
    "\n",
    "df_org_neutral.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df_org_neutral, values='count', names='organization',\n",
    "             title='Pie chart by Top organizations in the Neutral Sentiment Section',)\n",
    "fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The class of neutral feelings is always neutral; they are neither proponents nor opponents of the subject at hand.\n",
    "\n",
    "---\n",
    "* ### Organizations in Anti Sentiment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORGANIZATIONS INCLUDED IN ANTI SENTIMENT\n",
    "train_anti = train[train['sent_labels'] == 'Anti']\n",
    "tokens = nlp(''.join(str(train_anti['message'].tolist())))\n",
    "items = [word.text for word in tokens.ents]\n",
    "\n",
    "org_list = []\n",
    "for ent in tokens.ents:\n",
    "    if ent.label_ =='ORG':\n",
    "        org_list.append(ent.text)\n",
    "\n",
    "org_counts = Counter(org_list).most_common()\n",
    "\n",
    "df_org_anti = pd.DataFrame(org_counts, columns=['organization','count'])\n",
    "df_org_anti['organization'] = df_org_anti['organization'].str.capitalize()\n",
    "df_org_anti['organization'] = df_org_anti['organization'].str.upper()\n",
    "\n",
    "\n",
    "df_org_anti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df_org_anti, values='count', names='organization',\n",
    "             title='Pie chart by Top organizations in the Anti Sentiment Section',)\n",
    "fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The class of neutral feelings is always neutral; they are neither proponents nor opponents of the subject at hand.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Classification Models\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Classification Models ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we'll look at different models, try to explain them, and learn about their benefits and drawbacks. |\n",
    "\n",
    "---\n",
    "\n",
    "*  Decision Tree Classifier<a id='DS'></a>\n",
    "*  RandomForest Classifier<a id='random'></a>\n",
    "*  LinearSVC(Support Vector Classifier)<a id='svc'></a>\n",
    "*  LGBMClassifier(Light Gradient Boosting Machine Classifier)<a id='LGBM'></a>\n",
    "*  Logistic Regression\n",
    "*  Stochastic\n",
    "*  Support Vector Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DC'></a>\n",
    "\n",
    "#### 6.1 Decision Tree Classifier\n",
    "\n",
    "![1*bcLAJfWN2GpVQNTVOCrrvw.png](https://miro.medium.com/max/688/1*bcLAJfWN2GpVQNTVOCrrvw.png)\n",
    "\n",
    "\n",
    "* Decision Trees (DTs) are  non-parametric supervised learning methods used for classification and regression. Decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.\n",
    "\n",
    "* Decision tree builds classification or regression models in the form of a tree structure. It breaks down data by partitioning it into subsets after each decision while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches. Leaf node represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.\n",
    "\n",
    "#### Visual representation of a `Decision Tree`\n",
    "\n",
    "![1*WerHJ14JQAd3j8ASaVjAhw.jpeg](https://miro.medium.com/max/963/1*WerHJ14JQAd3j8ASaVjAhw.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "*Decision trees are prone to overfitting. Overfitting happens when the learning algorithm continues to develop hypotheses that reduce training set error at the cost of an increased test set error; One method to tackle overfitting in decision trees is by **prunning**.\n",
    "There are several approaches used to avoid overfitting in building decision trees namely, \t\t\n",
    "- Pre-pruning that stops growing the tree earlier, before it perfectly classifies the training set.\n",
    "- Post-pruning that allows the tree to perfectly classify the training set, and then post prune the tree. \n",
    "Practically, the second approach of post-pruning overfit trees is more successful because it is not easy to precisely estimate when to stop growing the tree.\n",
    "\n",
    "Decision Trees are building blocks for the next machine learning method we will look into, which is the **Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Data or feature Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section we would be: We will be looking at one of feature selection in text data namely the `tfidfVectorizer`. |\n",
    "\n",
    "---\n",
    "### 7.1 `TF-IDF`\n",
    "\n",
    "<a id='tfidf'></a>\n",
    "\n",
    "Term Frequency — Inverse Text Frequency (TF-IDF) is a statistic that attempts to better quantify how essential a term is for a document while also considering its relationship to other papers in the same corpus.\n",
    "This is done by counting the number of times a term appears in a document as well as the number of times the same word appears in other documents in the corpus.\n",
    "`vocabulary_` Is a dictionary that turns each word in the text into a feature index in the matrix, with a feature index for each unique token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -  Creating our X and y Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data['message']\n",
    "y = train_data['sentiment']\n",
    "X_test = test_data['message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Splitting data into train and validation sets\n",
    "\n",
    "Separating data into training and validation sets is an important part of evaluating our models. \n",
    "In our case we will randomly split the train data into 70% train and 30% validation. \n",
    "After our model is trained with the train data we then use it to make predictions for the target using the validation set,Because the data in the validation set already contains known values for the target variable this will make it easy  for us to asses our model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into 20% train and 30% validation set\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=.20,shuffle=True, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Pipelines'></a>\n",
    "\n",
    "### 6.2 Pipelines\n",
    "\n",
    "A 'pipeline' is a tool that applies a set of transformations and a final estimator in a sequential manner. The pipeline's intermediate phases implement fit and transform techniques, whereas the final estimator simply has to implement fit. Pipelines will assist us in transforming the train, validation, and test data, as well as training our models, in our scenario.\n",
    "\n",
    "Because our models can only process numerical data, the first step is to create a pipeline that converts text data to numeric data. In this notebook, we'll focus on two feature engineering methods that we'll use to convert text data to numeric data: TfidfVectorizer and Word2Vec, and then we'll train our models using these pipelines.\n",
    "\n",
    "We'll be utilizing 'tfidfVectorizer' to produce features for pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='classifiers'></a>\n",
    "\n",
    "#### 6.2.1 Building classification  pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier Pipeline\n",
    "tree = Pipeline([('tfidf', TfidfVectorizer()),('tree', DecisionTreeClassifier()),])\n",
    "\n",
    "\n",
    "# RandomForestClassifier Pipeline\n",
    "rfc = Pipeline([('tfidf', TfidfVectorizer()), ('rfc', RandomForestClassifier())])\n",
    "\n",
    "\n",
    "# LinearSVC Pipeline\n",
    "Lsvc = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('scv', LinearSVC()),])\n",
    " \n",
    "# LGBMClassifier Pipeline\n",
    "lgbm = Pipeline([('tfidf', TfidfVectorizer()), ('lgbm', LGBMClassifier())])\n",
    "\n",
    "# Logistic Regression pipeline\n",
    "logreg = Pipeline([('tfidf', TfidfVectorizer()),('logistic', LogisticRegression()),])\n",
    "\n",
    "\n",
    "# SGD Classifier pipeline\n",
    "SGD = Pipeline([('tfidf', TfidfVectorizer()), ('SGD', SGDClassifier())])\n",
    "\n",
    "# Support Vector Classifier Pipeline\n",
    "svc = Pipeline([('tfidf', TfidfVectorizer()), ('SVC', SVC())])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='raw_tweets'></a>\n",
    "\n",
    "#### 6.3 Training models\n",
    "\n",
    "Each model is trained using a unique pipeline that takes raw text input, converts it to numeric data, and sets the default parameters for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the decision tree pipeline\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# training the RandomForest pipleline\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# training the LinearSVC pipeline\n",
    "Lsvc.fit(X_train, y_train)  \n",
    "\n",
    "# training the LGBMClassifier Pipleine\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "# training the logistic regression pipeline\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# training the SGD Classifier\n",
    "SGD.fit(X_train, y_train)\n",
    "\n",
    "# training the support vector classifier\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eight\"></a>\n",
    "## 8. Modelling and Evaluation\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling and Evaluation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section we would be:we would be evaluating all the models. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Performance Metrics for model evaluation\n",
    "\n",
    "The F1 Score, which is the number of true cases for each label, will be used to assess our models.\n",
    "\n",
    "- ####  Precision\n",
    "The ratio of accurately predicted positive observations to total expected positive observations is known as precision.\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP \\space + FP} = \\frac{TP}{Total \\space Predicted \\space Positive} $$\n",
    "\n",
    "- #### Recall\n",
    "The capacity of the classifier to discover all the positive samples is known as recall.\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP \\space + FN} = \\frac{TP}{Total \\space Actual \\space Positive}$$\n",
    "\n",
    "- #### F1 Score\n",
    "In machine learning, the F1-score is one of the most crucial assessment measures. It effectively summarizes a model's prediction ability by combining two variables that would otherwise be at odds - precision and recall. \n",
    "\n",
    "(Weighted average of precision and recall.)\n",
    "\n",
    "$$F_1 = 2 \\times \\frac {Precision \\space \\times \\space Recall }{Precision \\space + \\space Recall }$$\n",
    "\n",
    "[2] \"News\": Tweets regarding climate change that are based on facts.\n",
    "\n",
    "[1] \"Pro\": Tweets in favor of man-made climate change.\n",
    "\n",
    "[0] \"Neutral\": tweets are those that neither support nor refute climate change theories.\n",
    "\n",
    "[-1] \"Anti\" : tweets are those who do not believe in man-made climate change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - To evaluate the base models we first start with making predictions for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making validations set predicions\n",
    "\n",
    "tree_prediction = tree.predict(X_val) # DecisionTreeClassifier predictions\n",
    "rfc_prediction = rfc.predict(X_val) # RandomForestClassifier predictions\n",
    "Lsvc_prediction = Lsvc.predict(X_val) # LinearSVClassifier Predictions\n",
    "lgbm_prediction = lgbm.predict(X_val) # LGBMClassifier Model predictions\n",
    "logreg_prediction = logreg.predict(X_val) # Logistic regression predictions\n",
    "SGD_prediction = SGD.predict(X_val) # SGD Classifier predictions\n",
    "SVC_prediction = svc.predict(X_val) # Support vector machine predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.1 Evalution of DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluatung with confusion matrix\n",
    "print(confusion_matrix(y_val, tree_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the results\n",
    "labels = ['-1:Anti', '0:Neutral', '1:Pro', '2:News']\n",
    "print('\\nDecision Tree\\n', classification_report(y_val, tree_prediction,target_names=labels))\n",
    "plot_confusion_matrix(y_val, tree_prediction, normalize=True,figsize=(8,8),cmap='Blues')\n",
    "plt.title('Decision Tree Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "A classification report is used to assess the accuracy of a classification algorithm's predictions.\n",
    "\n",
    "+ We can observe that pro climate change has a precision of 68 percent, which is higher, followed by news, neutral, and anti climate change.\n",
    "\n",
    "+ In comparison to the models we've tried, the precision for anti-climate change and neutral is lower.\n",
    "\n",
    "+ The pro-climate class has the greatest recall value of 73 percent.\n",
    "\n",
    "+ The recall of neutral is as low as 4%.\n",
    "\n",
    "\n",
    "\n",
    "The recall score for each sentiment class is shown in the confusion matrix heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visual represetation of of the f1 score for each class\n",
    "labels = ['-1:Anti', '0:Neutral', '1:Pro', '2:News']\n",
    "report_tree = classification_report(y_val, tree_prediction, output_dict=True,target_names=labels)\n",
    "df_tree = pd.DataFrame(report_tree).transpose()\n",
    "df_tree.drop(['accuracy'], inplace = True)\n",
    "df_tree.sort_values(by=['f1-score'],ascending=True, inplace = True)\n",
    "df_tree.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\n",
    "plt.xlabel('f1-score')\n",
    "plt.ylabel('Classes')\n",
    "plt.yticks(rotation = 40)\n",
    "plt.title('f1-score per sentiment class for Decision Tree Classiffier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "The f1 score for each sentiment class using the Decision Tree classifier is shown in the bar graph above.\n",
    "\n",
    "- We can observe that the decision tree model performed a great job classifying 'Pro climate change' tweets, followed by 'News' and 'Neutral' tweets, in that order.\n",
    "\n",
    "- With a f1 score of less than 0.3, the Decision Tree classifier performed a bad job classifying 'Anti Climate Change' tweets.\n",
    "\n",
    "- Given the imbalance in our train data, where 'Anti climate change' tweets only account for 8% of all tweets in the train dataset, poor categorization of 'Anti climate change' tweets is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the overall accuracy\n",
    "\n",
    "decison_tree_acc = round(accuracy_score(y_val, tree_prediction),4)\n",
    "print(f'\\nOverall Accuracy score for Decision Tree : {decison_tree_acc}')\n",
    "decision_tree_f1 = round(f1_score(y_val, tree_prediction, average=\"weighted\"),4)\n",
    "print(f'\\nWeighted avg f1 score Decision Tree {decision_tree_f1}' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.2  Evalution of RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluatung with confusion matrix\n",
    "print('\\nRandomForestClassifier\\n', confusion_matrix(y_val, rfc_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['-1:Anti', '0:Neutral', '1:Pro', '2:News']\n",
    "print('\\nRandomForestClassifier\\n', classification_report(y_val, rfc_prediction,target_names=labels))\n",
    "plot_confusion_matrix(y_val, rfc_prediction, normalize=True,figsize=(8,8),cmap='Blues')\n",
    "plt.title('Random Forest Classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Observations\n",
    "\n",
    "- We can observe that anti-climate has a precision of 91 percent, neutral has a precision of 72 percent, and news has a precision of 76 percent, which is greater than 'Decision Tree.'\n",
    "\n",
    "- When compared to 'Decision Tree,' the recall values for the pro-climate class are 91 percent and news is 70 percent.\n",
    "\n",
    "- When compared to 'Decision Tree,' anti-climate recalls decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visual represetation of of the f1 score for each class\n",
    "labels = ['-1:Anti', '0:Neutral', '1:Pro', '2:News']\n",
    "report_rfc = classification_report(y_val, rfc_prediction, output_dict=True,target_names=labels)\n",
    "df_rfc = pd.DataFrame(report_rfc).transpose()\n",
    "df_rfc.drop(['accuracy'], inplace = True)\n",
    "df_rfc.sort_values(by=['f1-score'],ascending=True, inplace = True)\n",
    "df_rfc.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\n",
    "plt.xlabel('f1-score')\n",
    "plt.ylabel('Classes')\n",
    "plt.yticks(rotation = 40)\n",
    "plt.title('f1-score per sentiment class for Random Forest Classiffier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Points to Consider\n",
    "\n",
    "The f1 score for each sentiment class using the Random Forest classifier is shown in the graph above.\n",
    "\n",
    "- We can observe that the Random Forest model performed a better job classifying 'Pro climate change' tweets and 'News' tweets than the Decision tree model, with both sentimetents scoring over 0.7. with 'Neutral' tweets following after that.\n",
    "\n",
    "- With a f1 score lower than the one we acquired using the 'Decision Tree' classifier, the Random Forest classifier performed a bad job classifying 'Anti climate Change' tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_acc = round(accuracy_score(y_val, rfc_prediction),4)\n",
    "print(f'\\nOveral accuracy score for RandomForestClassifier :{random_forest_acc}')\n",
    "random_forest_f1 = round(f1_score(y_val, rfc_prediction, average=\"weighted\"),4)\n",
    "print(f'\\nWeighted f1 score for RandomForestClassifier : {random_forest_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.3 Evaluation of LinearSVClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLinearSVC Model\\n', confusion_matrix(y_val, Lsvc_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['-1:Anti', '0:Neutral', '1:Pro', '2:News']\n",
    "print('\\nLinearSVC Model\\n', classification_report(y_val, Lsvc_prediction,target_names=labels))\n",
    "plot_confusion_matrix(y_val, Lsvc_prediction, normalize=True,figsize=(8,8),cmap='Blues')\n",
    "plt.title('LinearSCV Classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Areas to Note\n",
    "\n",
    "+ We can observe that anti climate has an accuracy of 69 percent, which is greater than 'Decision Tree' but lower than 'Random Forest.'\n",
    "\n",
    "+ Pro climate has a precision of 69 percent, which is greater than 'Decision Tree' and 'Random Forest.'\n",
    "\n",
    "+ When compared to 'Decision Tree' and 'Random Forest,' the news class has the highest recall score of 81 percent.\n",
    "+ When compared to 'Random Forest,' the pro-climate class had a lower recall value. When compared to 'Random Forest' and 'Decision Tree,' anti-climate recall rose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visual represetation of of the f1 score for each class\n",
    "labels = ['-1:Anti', '0:Neutral', '1:Pro', '2:News']\n",
    "report_svc = classification_report(y_val, Lsvc_prediction, output_dict=True,target_names=labels)\n",
    "df_svc = pd.DataFrame(report_svc).transpose()\n",
    "df_svc.drop(['accuracy'], inplace = True)\n",
    "df_svc.sort_values(by=['f1-score'],ascending=True, inplace = True)\n",
    "df_svc.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\n",
    "plt.xlabel('f1-score')\n",
    "plt.ylabel('Classes')\n",
    "plt.yticks(rotation = 40)\n",
    "plt.title('f1-score per sentiment class for LinearSVC ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Observations\n",
    "\n",
    "Using the LinearSVC, the f1 score for each sentiment class is shown in the bar graph above.\n",
    "\n",
    "- We can observe that the LinearSVC model did a far better job at classifying 'Pro climate change' tweets than the 'Decision Tree' and 'RandomForest' models, with a f1 score of above 0.8 for the label 1 sentiment class.\n",
    "\n",
    "- When compared to both the Decision tree and RandomForest models, the LinearSVC model did a considerably better job identifying 'News' tweets, with the highest score level 2 sentiment class of over 0.75.\n",
    "\n",
    "- We also notice a significant increase in the categorization of \"anti-climate change\" tweets, with a f1 score of just over 0.5, up from just under 0.3 using the \"Decision Tree\" classifier.\n",
    "\n",
    "- The LinearSVC made a minor improvement in the categorization of 'neutral' tweets, but it was considerably outweighed by the gains in other sentiments classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearSVC_acc = round(accuracy_score(y_val, Lsvc_prediction),4)\n",
    "print(f'\\nOverall accuracy score for LinearSVC Model : {linearSVC_acc}')\n",
    "linearSVC_f1 = round(f1_score(y_val, Lsvc_prediction, average=\"weighted\"),4)\n",
    "print(f'\\nWeighted avg f1 score for LinearSVC Model : {linearSVC_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.4 Evaluation of LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLightGBM\\n', confusion_matrix(y_val, lgbm_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['-1:Anti', '0:Neutral', '1:Pro', '2:News']\n",
    "print('\\nLightGBM\\n', classification_report(y_val, lgbm_prediction,target_names=labels))\n",
    "plot_confusion_matrix(y_val, lgbm_prediction, normalize=True,figsize=(8,8),cmap='Blues')\n",
    "plt.title('LightGBM Classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Points to Consider\n",
    "\n",
    "+ When compared to 'LinearSVC' and 'Random Forest,' we can find that anti climate and neutral accuracy has reduced by 66%. However, when compared to 'Decision Tree,' it has risen.\n",
    "\n",
    "+ When compared to 'LinearSVC' and 'Random Forest,' the recall values for the pro climate class of 84 percent have decreased, but they have increased when compared to 'Decision Tree.'\n",
    "+ In comparison to 'Random Forest,' 'LinearSVC,' and 'Decision Tree,' the recall of the neutral class rose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visual represetation of of the f1 score for each class\n",
    "labels = ['-1:Anti', '0:Neutral', '1:Pro', '2:News']\n",
    "report_lgbm = classification_report(y_val, lgbm_prediction, output_dict=True,target_names=labels)\n",
    "df_lgbm = pd.DataFrame(report_lgbm).transpose()\n",
    "df_lgbm.drop(['accuracy'], inplace = True)\n",
    "df_lgbm.sort_values(by=['f1-score'],ascending=True, inplace = True)\n",
    "df_lgbm.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\n",
    "plt.xlabel('f1-score')\n",
    "plt.ylabel('Classes')\n",
    "plt.yticks(rotation = 40)\n",
    "plt.title('f1-score per sentiment class for LightGBM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Points to Consider\n",
    "\n",
    "The f1 score for each sentiment class is depicted in the bar graph above using the LightGBM.\n",
    "\n",
    "- Despite outperforming the 'Decision Tree' and 'Random Forest' models in identifying 'Pro climate tweets' and 'News,' the LightGBM model still falls short of the 'LinearSVC' model.\n",
    "\n",
    "- We see a little increase in the classification of 'neutral' tweets when compared to the categorization of 'LinearSVC' tweets.\n",
    "\n",
    "- Although 'LightGBM' has a lower categorization of 'Anti climate change' tweets than 'LinearSVC,' it still outperforms the first two models we looked at, 'Decision Tree' and 'Random Forest.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lGBM_acc = round(accuracy_score(y_val, lgbm_prediction),4)\n",
    "print(f'\\nOverall accuracy score for LightGBM :{lGBM_acc}')\n",
    "lGBM_f1 = round(f1_score(y_val, lgbm_prediction, average=\"weighted\"),4)\n",
    "print(f'\\nWeighted f1 score for LightGBM :{lGBM_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.5 Evaluation of  Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the confusion matrix\n",
    "print('\\nLogistic Regression\\n', confusion_matrix(y_val, logreg_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a classification report\n",
    "labels = ['-1:Anti', '0:Neutral', '1:Pro', '2:News']\n",
    "print('\\nLogistic Regression\\n', classification_report(y_val, logreg_prediction,target_names=labels))\n",
    "plot_confusion_matrix(y_val, logreg_prediction, normalize=True,figsize=(8,8),cmap='Blues')\n",
    "plt.title('Logistic Regression Classification')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider the Following Points\n",
    "\n",
    "+ Anti climate has a 76 percent accuracy, which is higher than 'Decision Tree,' 'LinearSVC,' and 'LGBMClassifier,' but not higher than 'Random Forest.'\n",
    "\n",
    "+ When compared to 'Decision Tree,' 'LinearSVC,' and 'LGBMClassifier,' the pro climate class has the highest recall values of 89 percent, but it is not higher than 'Random Forest.'\n",
    "\n",
    "+ In \"recall,\" anti climate is the same as 'LGBMClassifier,' and it has risen in comparison to 'Random Forest' and 'Logistic Regression,' but has fallen in comparison to 'LinearSVC' and 'Decision Tree.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visual represetation of of the f1 score for each class\n",
    "labels = ['-1:Anti', '0:Neutral', '1:Pro', '2:News']\n",
    "report_logreg = classification_report(y_val, logreg_prediction, output_dict=True,target_names=labels)\n",
    "df_logreg = pd.DataFrame(report_logreg).transpose()\n",
    "df_logreg.drop(['accuracy'], inplace = True)\n",
    "df_logreg.sort_values(by=['f1-score'],ascending=True, inplace = True)\n",
    "df_logreg.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\n",
    "plt.xlabel('f1-score')\n",
    "plt.ylabel('Classes')\n",
    "plt.yticks(rotation = 40)\n",
    "plt.title('f1 score per sentiment class for Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### observations (Type here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the overall accuracy\n",
    "logistic_reg_acc = round(accuracy_score(y_val, logreg_prediction),4)\n",
    "print('\\nLogistic Regression accuracy Score\\n', logistic_reg_acc)\n",
    "logistic_reg_f1 = round(f1_score(y_val, logreg_prediction, average=\"weighted\"),4)\n",
    "print('\\nLogistic Regression weighted f1 score\\n', logistic_reg_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.6 Evaluation of SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for the random forest classifier\n",
    "print('\\nSGD Classifier\\n', confusion_matrix(y_val, SGD_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classification report \n",
    "labels = ['-1:Anti', '0:Neutral', '1:Pro', '2:News']\n",
    "print('\\nSGD Classifier  Classification report :\\n', classification_report(y_val, SGD_prediction,target_names=labels))\n",
    "plot_confusion_matrix(y_val, SGD_prediction, normalize=True,figsize=(8,8),cmap='Blues')\n",
    "plt.title('Logistic Regression Classification')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Observations\n",
    "\n",
    "+ When compared to the 'Decision Tree,' the precision value for the news class decreased.\n",
    "\n",
    "+ Anti and neutral are more accurate than 'Decision Tree,' 'LinearSVC,' LGBMClassifier,' and 'Logistic Regression.' It's not quite as gloomy as 'Random Forest,' but it comes close.\n",
    "\n",
    "+ -The recall value for the pro climate class is 88 percent, which is greater than 'Random Forest,' and 'Logistic Regression,' when compared to 'Decision Tree,' 'LinearSVC,' and 'LGBMClassifier.'\n",
    "\n",
    "+ Anti climate has a greater recall than 'Decision Tree,' 'Random Forest,' 'LGBMClassifier,' and 'Logistic Regression,' but not as high as 'LinearSVC.'\n",
    "\n",
    "+ Neutral's recall is lower than that of 'Decision Tree,' 'LinearSVC,' and 'LGBMClassifier,' but greater than that of 'Random Forest,' and 'Logistic Regression.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visual represetation of of the f1 score for each class\n",
    "labels = ['-1:Anti', '0:Neutral', '1:Pro', '2:News']\n",
    "report_sgd = classification_report(y_val, SGD_prediction, output_dict=True,target_names=labels)\n",
    "df_sgd = pd.DataFrame(report_sgd).transpose()\n",
    "df_sgd.drop(['accuracy'], inplace = True)\n",
    "df_sgd.sort_values(by=['f1-score'],ascending=True, inplace = True)\n",
    "df_sgd.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\n",
    "plt.xlabel('f1-score')\n",
    "plt.ylabel('Classes')\n",
    "plt.yticks(rotation = 40)\n",
    "plt.title('f1-score per sentiment class for SGD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the accuracy score\n",
    "sgd_acc = round(accuracy_score(y_val, SGD_prediction),4)\n",
    "print('\\nSGD Classifier accuracy Score :\\n', sgd_acc)\n",
    "\n",
    "# Checking the f1_score report for the decison tree model\n",
    "sgd_f1 = round(f1_score(y_val, SGD_prediction, average=\"weighted\"),4)\n",
    "print('\\nSGD weighted avg f1_score :\\n', sgd_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.7 Support Vector Classfifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for the Support Vector Classifier\n",
    "print('\\nSupport Vector Classifier\\n', confusion_matrix(y_val, SVC_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classification report \n",
    "labels = ['-1:Anti', '0:Neutral', '1:Pro', '2:News']\n",
    "print('\\nSupport Vector Classifier  Classification report :\\n', classification_report(y_val, SVC_prediction),target_names=labels)\n",
    "plot_confusion_matrix(y_val, SVC_prediction, normalize=True,figsize=(8,8),cmap='Blues')\n",
    "plt.title('Support Vector Classification')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Points to Keep in Mind\n",
    "\n",
    "+ When compared to 'Decision Tree,' 'LinearSVC,' LGBMClassifier,'Logistic Regression,' and 'SGD Classifier,' anti climate has a greater accuracy. It's nothing more than the 'Random Forest' anti-climate.\n",
    "\n",
    "\n",
    "+ The pro climate class has a recall value of 91%, which is the same as 'Random Forest' and higher than 'Decision Tree,'LinearSVC,'LGBMClassifier,'Logistic Regression,' and 'SGD Classifier.'\n",
    "\n",
    "+ Anti climate has a lower recall than Decision Tree, LinearSVC, LGBMClassifier, Logistic Regression, and SGD Classifier, but it has a higher recall than Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visual represetation of of the f1 score for each class\n",
    "labels = ['-1:Anti', '0:Neutral', '1:Pro', '2:News']\n",
    "report_svc = classification_report(y_val, SVC_prediction, output_dict=True, target_names=labels)\n",
    "df_SVC = pd.DataFrame(report_svc).transpose()\n",
    "df_SVC.drop(['accuracy'], inplace = True)\n",
    "df_SVC.sort_values(by=['f1-score'],ascending=True, inplace = True)\n",
    "df_SVC.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\n",
    "plt.xlabel('f1-score')\n",
    "plt.ylabel('Classes')\n",
    "plt.yticks(rotation = 40)\n",
    "plt.title('f1-score per sentiment class for SVC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Points to Remember\n",
    "Using the Support Vector Classifier, the above bar graph depicts the f1 score for each sentiment class (SVC)\n",
    "\n",
    "- Similar to the 'LinearSVC,' the 'SVC' performs an excellent job of identifying the 'Pro climate change' sentiment class with a score of 0.8, followed by the 'News' sentiment class with a f1 score of over 0.75.\n",
    "\n",
    "- The 'Support Vector Classifier,' like the most of the models we've looked at so far, had trouble categorizing the 'anti climate change' attitude, scoring just over 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Checking the accuracy score\n",
    "svc_acc = round(accuracy_score(y_val, SVC_prediction),4)\n",
    "print(f'\\nSupport Vector Classifier accuracy Score :{svc_acc}')\n",
    "svc_f1 = round(f1_score(y_val, SVC_prediction, average=\"weighted\"),4)\n",
    "print(f'\\nSupport Vector Classifier weighted avg f1_score :{svc_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Model Comparision\n",
    "\n",
    "Model comparison by accuracy and macro f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with our models and their performances metrics\n",
    "classifier_scores = {'Classifiers':['Decision Tree', 'Random Forest','LinearSVC',\n",
    "                                    'LGBM','Logistic Regression','Stochastic Gradient Descent','Support Vector Classifier'],\n",
    "                    'Accuracy':[decison_tree_acc,random_forest_acc,\n",
    "                                linearSVC_acc,lGBM_acc,logistic_reg_acc, sgd_acc, svc_f1],\n",
    "                     'Weighted avg f1 Score':[decision_tree_f1,random_forest_f1,\n",
    "                                       linearSVC_f1,lGBM_f1,logistic_reg_f1, sgd_f1, svc_f1]}\n",
    "df= pd.DataFrame(classifier_scores)\n",
    "df.sort_values(by=['Accuracy'],ascending=True, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(df['Classifiers'], inplace = True)\n",
    "df.drop(['Classifiers'],axis = 1)\n",
    "df.plot(kind='barh', figsize = (8,8),colormap='coolwarm_r')\n",
    "plt.xlabel('Score')\n",
    "plt.yticks(rotation = 45)\n",
    "plt.title('Classifier Perfomance')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a comparison of the 7 models we've tried so far based on their 'accuracy score' and accompanying 'weighted f1 score' in the upper bar graph.\n",
    "\n",
    "- We can see that our top three best performing models are 'LinearSVC,\"Stochastic Gradient Descent,' and 'Logistic Regression,' which we will apply in ensemble approaches to try to enhance our findings.\n",
    "\n",
    "- The 'Decision Tree' classifier performs the poorest at categorizing tweets, with 0.61 and 0.60 weighted f1 scores, respectively.\n",
    "\n",
    "\n",
    "\n",
    "**With an accuracy score of 0.74 and a weighted f1 score of 0.72, LinearSVC is the highest performing model out of the 7 models we've tried so far**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Ensemble Methods\n",
    "\n",
    "Ensemble learning is the practice of combining many models to improve overall model performance in machine learning. Ensembles are made up of several **heterogeneous or homogeneous** models that have all been trained on the same dataset. Each of these models is used to make predictions based on the same input, which are then combined in some way (for example, by taking the mean) to provide the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ensemble'></a>\n",
    "\n",
    "#### 6.6.1 Method of Heterogeneous Ensembel\n",
    "This sort of ensemble comprises of several types of models, so we can use it to include pretty much any classification model we want. In our instance, however, we'll just use our top three performing models, which are 'LinearSVC, Stochastic Gradient Descent, Logistic Regression.'\n",
    "\n",
    "The 'Voting classifier' is the Heterogeneous ensemble approach we'll look at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Voting classifer \n",
    "\n",
    "Voting involves combining individual model outputs through a kind of \"[majority rule](https://en.wikipedia.org/wiki/Majority_rule)\" paradigm.\n",
    "The diagram below shows how the `Voting Classifier` works\n",
    "![ud382N9.png](https://i.ibb.co/YjsbSYN/ud382N9.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models which we'll include in our ensemble. \n",
    "# We pass a list of tuples, which each have a string identifier for the\n",
    "# model (arbitrary choice), along the actual instantiated sklearn model.  \n",
    "models = [(\"LinearSVC\",Lsvc),(\"SGD\",SGD),(\"Logistric Regression\",logreg)]\n",
    "\n",
    "# Specify weights for weighted model averaging\n",
    "model_weightings = np.array([0.1,0.3,0.6])\n",
    "\n",
    "# building the voting classifier\n",
    "Voting_classifier = VotingClassifier(estimators=models,weights=model_weightings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the voting classifier\n",
    "Voting_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_prediction = Voting_classifier.predict(X_val) # Voting Classifier predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classification report \n",
    "print('\\nVoting Classifier  Classification report :\\n', classification_report(y_val, voting_prediction))\n",
    "plot_confusion_matrix(y_val, voting_prediction, normalize=True,figsize=(8,8),cmap='Blues')\n",
    "plt.title('Voting Classifier Classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual represetation of of the f1 score for each class\n",
    "report_voting = classification_report(y_val, voting_prediction, output_dict=True)\n",
    "df_voting = pd.DataFrame(report_voting).transpose()\n",
    "df_voting.drop(['accuracy'], inplace = True)\n",
    "df_voting.sort_values(by=['f1-score'],ascending=True, inplace = True)\n",
    "df_voting.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\n",
    "plt.xlabel('f1-score')\n",
    "plt.ylabel('Classes')\n",
    "plt.yticks(rotation = 40)\n",
    "plt.title('f1-score per sentiment class for the voting classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the accuracy score\n",
    "voting_acc = round(accuracy_score(y_val, voting_prediction),4)\n",
    "print(f'\\nOverall accuracy for the Voting Classifier :{voting_acc}')\n",
    "voting_f1 = round(f1_score(y_val, voting_prediction, average=\"weighted\"),4)\n",
    "print(f'\\nWeighted avg f1 score for the Voting Classifier :{voting_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations (Type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.6.2 Homogeneous Ensembles\n",
    "These are all models of the same kind. As a result, controlling the number of predictors or the percentage of data provided to each model in the ensemble is a typical technique to create variety in model performance in these circumstances. This is similar to what happens in a random forest.\n",
    "\n",
    "**Bagging** and **boosting** are two of the most prevalent strategies for merging models in this fashion.\n",
    "\n",
    "* #### Bagging (AKA Bootstrap Aggregating)\n",
    "\n",
    "Bagging is the process of training the ensemble's models on distinct subsets of the training data. Particularly on subsets from the training data that are **sampled with replacement**. As a result of the reduced variance error, the resulting 'bag' of models is more stable overall.\n",
    "\n",
    "All of the models in the bag's forecasts are combined to make the projections.\n",
    "\n",
    "![1*JksRZ1E72Rsx2s8lQbNR1w.jpeg](https://miro.medium.com/max/875/1*JksRZ1E72Rsx2s8lQbNR1w.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_class= Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('bag', BaggingClassifier(base_estimator = LinearSVC())),])\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_class.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_prediction = bag_class.predict(X_val) # Bagging Classifier predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classification report \n",
    "print('\\nBagging Classifier  Classification report :\\n', classification_report(y_val, bag_prediction))\n",
    "plot_confusion_matrix(y_val, voting_prediction, normalize=True,figsize=(8,8),cmap='Blues')\n",
    "plt.title('Bagging Classifier Classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual represetation of of the f1 score for each class\n",
    "report_bagging = classification_report(y_val, bag_prediction, output_dict=True)\n",
    "df_bag = pd.DataFrame(report_bagging).transpose()\n",
    "df_bag.drop(['accuracy'], inplace = True)\n",
    "df_bag.sort_values(by=['f1-score'],ascending=True, inplace = True)\n",
    "df_bag.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\n",
    "plt.xlabel('f1-score')\n",
    "plt.ylabel('Classes')\n",
    "plt.yticks(rotation = 40)\n",
    "plt.title('f1-score per sentiment class for the voting classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the accuracy score\n",
    "bag_acc = round(accuracy_score(y_val, bag_prediction),4)\n",
    "print(f'\\nBagging Classifier accuracy Score :{bag_acc}')\n",
    "bag_f1 = round(f1_score(y_val, bag_prediction, average=\"weighted\"),4)\n",
    "print(f'\\nBagging Classifier weighted avg f1 score :{bag_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hy_tunning'></a>\n",
    "\n",
    "### 6.3 Hyperparameter Tuning\n",
    "\n",
    "We'll use the GridSearchCV approach to find the optimum hyperparameters for our models.\n",
    "\n",
    "* Models on which we will apply hyperparameter tweaking\n",
    "\n",
    "  * LinearSVC\n",
    "\n",
    "  * Logistic Regression\n",
    "\n",
    "  * Support Vector Classifier\n",
    "\n",
    "The disadvantage of using pipelines to build our models is that we can't easily get the parameters for our models. As a result, we won't be using pipelines to perform hyperparameter tuning and obtain the best parameters for our models. This means we'll have to convert raw text data to numeric using 'TifidfVectorizer' separately from building the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "#Change 1\n",
    "X_train_new= X_train.copy()\n",
    "X_val_new = X_val.copy()\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,20), min_df=2)\n",
    "vectorizer.fit(X_train)\n",
    "\n",
    "X_train_new = vectorizer.transform(X_train)\n",
    "X_val_new = vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hyperprarameter gridsearch for the LinearSVC is purposefully commented out because \n",
    "# it perfoms 5 folds for each of the 9 candidates totalling in 45 fits, this GridSearch code took 27 minutes to complete\n",
    "\n",
    "#  we have saved the resulting model as a pickle file for convinience\n",
    "\n",
    "\n",
    "'''\n",
    "param_grid = {'C': [0.2,0.3,0.4,0.5,1.0,3,3.01,10, 100]}\n",
    "# grid_lsvc = GridSearchCV(LinearSVC(),param_grid,refit=True,verbose=2)\n",
    "# grid_lsvc.fit(X_train_new,y_train)\n",
    "\n",
    "grid_lsvc = Pipeline([('tfidf', TfidfVectorizer()), ('grid', GridSearchCV(SVC(),\n",
    "                                                                     param_grid,refit=True,verbose=2))])\n",
    "                                                                     \n",
    "\n",
    "grid_lsvc.fit(X_train,y_train)\n",
    "\n",
    "# Saving the model\n",
    "import pickle\n",
    "model_save_path = 'LinearSVC.pkl'\n",
    "with open(model_save_path, 'wb') as file:\n",
    "    pickle.dump(grid_lsvc, file)\n",
    "    \n",
    "    \n",
    "''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # loading the saved Logistic Regression model\n",
    "model_load_path = 'LinearSVC.pkl'\n",
    "with open(model_load_path, 'rb') as file:\n",
    "    grid_lsvc=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_lsvc_predictions = grid_lsvc.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the accuracy score\n",
    "tuned_lsvc_acc = round(accuracy_score(y_val, tuned_lsvc_predictions),4)\n",
    "print(f'\\nOverall accuracy score for LinearSVC :{tuned_lsvc_acc}')\n",
    "tuned_lsvc_f1 = round(f1_score(y_val, tuned_lsvc_predictions, average=\"weighted\"),4)\n",
    "print(f'\\nWeighted f1 score for LinearSVC :{tuned_lsvc_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hyperprarameter gridsearch for the logistic regression model is purposefully commented out because \n",
    "# it perfoms 5 folds for each of the 50 candidates totalling in 250 fits, this GridSearch  code took 3.6 minutes to complete\n",
    "#  we have saved the resulting model as a pickle file\n",
    "\n",
    "\n",
    "'''\n",
    "param_grid = [\n",
    "    {'penalty' : [ 'l2'],\n",
    "    'C' : np.logspace(-4, 4, 50),\n",
    "    'solver' : ['lbfgs']},]\n",
    "\n",
    "Logistic_reg_grid = GridSearchCV(LogisticRegression(),param_grid,refit=True,verbose=2)\n",
    "Logistic_reg_grid.fit(X_train_new, y_train)\n",
    "\n",
    "\n",
    "# Saving the model\n",
    "import pickle\n",
    "model_save_path = 'LogisticReg.pkl'\n",
    "with open(model_save_path, 'wb') as file:\n",
    "    pickle.dump(logistic_grid, file)\n",
    "    \n",
    "  \n",
    " # loading the saved Logistic Regression model\n",
    "model_load_path = 'LogisticReg.pkl'\n",
    "with open(model_load_path, 'rb') as file:\n",
    "    Logistic_reg_tuned=pickle.load(file)\n",
    "    \n",
    "'''    \n",
    "param_dict = {'C': \"1000\", 'penalty': 'l2', \"multi_class\" :'ovr','solver': 'saga'}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining the Logistic Regresion model with best parameters                     \n",
    "logreg_tunned = Pipeline([('tfidf', TfidfVectorizer()),('logistic', LogisticRegression(C=1000, multi_class='ovr', \n",
    "                                                                          solver='saga', random_state=11, max_iter=10)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_tunned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunned_logreg_acc = round(accuracy_score(y_val,logreg_tunned.predict(X_val)),4)\n",
    "print(f'Overall accuracy score Tuned Logistic Regression accuracy Score :{tunned_logreg_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunned_logreg_f1 = round(f1_score(y_val, logreg_tunned.predict(X_val), average = 'weighted'),4)\n",
    "print(f'Weighted avg f1 score for Tuned Logistic Regression Classifier :{tunned_logreg_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is purposefully commnted out because it Fits 5 folds for each of 72 candidates, totalling 360 fits\n",
    "# the total runtime for this gridserch was 186 Minutes\n",
    "# we have saved the model in a pickle file\n",
    "\n",
    "\n",
    "# param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "# grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n",
    "\n",
    "# grid = Pipeline([('tfidf', TfidfVectorizer()), ('grid', GridSearchCV(SVC(),\n",
    "#                                                                      param_grid,refit=True,verbose=2))])\n",
    "                                                                     \n",
    "\n",
    "\n",
    "# param_grid = {'C': [0.1,1,3,3.01,10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "# grid = Pipeline([('tfidf', TfidfVectorizer()), ('grid', GridSearchCV(SVC(),\n",
    "#                                                                      param_grid,refit=True,verbose=2))])\n",
    "\n",
    "\n",
    "\n",
    "                                                                     \n",
    "                                                                                                                                 \n",
    "# # training the tunned model\n",
    "# grid.fit(X_train, y_train) \n",
    "\n",
    "\n",
    "# # Saving the model\n",
    "# import pickle\n",
    "# model_save_path = 'SVCGrid.pkl'\n",
    "# with open(model_save_path, 'wb') as file:\n",
    "#   pickle.dump(grid, file)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the saved model\n",
    "model_load_path = 'SVCGrid.pkl'\n",
    "with open(model_load_path, 'rb') as file:\n",
    "    TunedSVC=pickle.load(file)\n",
    "TunedSVC_prediction =TunedSVC.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the accuracy score\n",
    "tunned_svc_acc = round(accuracy_score(y_val, TunedSVC_prediction),4)\n",
    "print(f'\\nOverall accuracy score for Tuned Support Vector Classifier accuracy Score :{tunned_svc_acc}')\n",
    "tunned_svc_f1 = round(f1_score(y_val, TunedSVC_prediction, average=\"weighted\"),4)\n",
    "print(f'\\nWeighted avg f1 score for Tuned Support Vector Classifier :{tunned_svc_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model selection\n",
    "\n",
    "Comparing all the models we've build so far to choose the best performing one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_scores = {'Classifiers':['Decision Tree', 'Random Forest','LinearSVC',\n",
    "                                    'LGBM','Logistic Regression','Stochastic Gradient Descent',\n",
    "                                    'Support Vector Classifier', 'Voting Classifer','Bagging Classifier',\n",
    "                                    'Tunned_LinearSVC','Tunned LogisticReg','Tunned_SVC'],\n",
    "                    'Accuracy':[decison_tree_acc,random_forest_acc,\n",
    "                                linearSVC_acc,lGBM_acc,logistic_reg_acc, sgd_acc, svc_f1, voting_acc,\n",
    "                                bag_acc,tuned_lsvc_acc ,tunned_logreg_acc,\n",
    "                                tunned_svc_acc],\n",
    "                     'Weighted avg f1 Score':[decision_tree_f1,random_forest_f1,\n",
    "                                       linearSVC_f1,lGBM_f1,logistic_reg_f1, sgd_f1, svc_f1,\n",
    "                                              voting_f1, bag_f1, tuned_lsvc_f1 ,tunned_logreg_f1, \n",
    "                                              tunned_svc_f1]}\n",
    "df= pd.DataFrame(classifier_scores)\n",
    "df.sort_values(by=['Accuracy'],ascending=True, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(df['Classifiers'], inplace = True)\n",
    "df.drop(['Classifiers'],axis = 1)\n",
    "df.plot(kind='barh', figsize = (8,8),colormap='winter')\n",
    "plt.xlabel('Score')\n",
    "plt.yticks(rotation = 45)\n",
    "plt.title('Classifier Perfomance')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final evaluation of our best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have build a total of 11 models in this notebook out of all the models we've build, We see that the best performing model is the tunned Support Vector Classifer with the best accuracy score of 0.82 and the best weighted f1 score  of 82.\n",
    "\n",
    "We will be using the **Support Vector Classifer** to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('classfication report for our best model\\n',classification_report(y_val, TunedSVC.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visual represetation of of the f1 score for each class\n",
    "report_tuned_svc = classification_report(y_val, TunedSVC_prediction, output_dict=True)\n",
    "df_tuned_svc = pd.DataFrame(report_tuned_svc).transpose()\n",
    "df_tuned_svc.drop(['accuracy'], inplace = True)\n",
    "df_tuned_svc.sort_values(by=['f1-score'],ascending=True, inplace = True)\n",
    "df_tuned_svc.drop(['weighted avg','macro avg'])['f1-score'].plot(kind='barh', figsize = (8,8))\n",
    "plt.xlabel('f1-score')\n",
    "plt.ylabel('Classes')\n",
    "plt.yticks(rotation = 40)\n",
    "plt.title('f1-score per sentiment class for tunned SVC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Support Vector Classifier is by far our best performing model, with a f1 score of 0.87 for the 'Pro climate change' sentiment class, followed by f1 scores of 0.84 and 0.70 for the 'News' and 'Anti' Climate sentiment classes, respectively, which is quite impressive given that all of our models performed poorly when it came to classifying the 'anti climate change' sentiment class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneVsRest =  OneVsRestClassifier(SVC(class_weight='balanced'))\n",
    "y_train_binarized = label_binarize(y_train, classes=[-1, 0, 1, 2])\n",
    "y_val_binarized = label_binarize(y_val, classes=[-1, 0, 1, 2])\n",
    "OneVsRest.fit(X_train_new, y_train_binarized)\n",
    "y_prob = OneVsRest.decision_function(X_val_new)\n",
    "plot_roc(y_val, y_prob,figsize=(10,10),cmap='cool')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='predictions'></a>\n",
    "# Final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Lsvc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['sentiment'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['tweetid','sentiment']].to_csv('SubmissionCSV.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['tweetid','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_val, y_pred, average='macro')\n",
    "precision = precision_score(y_val, y_pred, average='macro')\n",
    "recall = recall_score(y_val, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"random_state\": 42,\n",
    "          \"model_type\": \"logreg\",\n",
    "          \"vectorizer\": \"tfidf_Vectorizer\",\n",
    "          \"imbalanced\": \"smote\",\n",
    "          \"param_grid\": str(param_dict),\n",
    "          \"stratify\": True\n",
    "          }\n",
    "metrics = {\"f1\": f1,\n",
    "           \"recall\": recall,\n",
    "           \"precision\": precision\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment.log_parameters(params)\n",
    "experiment.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing the comet experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the experiment\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF-IDF Explained And Python Sklearn Implementation :\n",
    "https://towardsdatascience.com/tf-idf-explained-and-python-sklearn-implementation-b020c5e83275\n",
    "\n",
    "* Decision Trees Explained Easily :\n",
    "https://medium.com/@chiragsehra42/decision-trees-explained-easily-28f23241248\n",
    "\n",
    "* Understanding Random Forests Classifiers in Python :\n",
    "https://www.datacamp.com/community/tutorials/random-forests-classifier-python\n",
    "\n",
    "* What is LightGBM, How to implement it? How to fine tune the parameters?\n",
    "https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n",
    "\n",
    "* How to make SGD Classifier perform as well as Logistic Regression\n",
    "https://towardsdatascience.com/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-VqeCRDaVxXY",
    "x9HLGvDNVxYM",
    "3TPRVVKQVxYO",
    "7rdfD0TkVxYO",
    "qL8jpnPxVxYP",
    "UCplaYUjVxYQ",
    "5QJEmvWVVxYQ",
    "LvDcaHe-VxYU",
    "25NQkdQ6VxYX"
   ],
   "name": "Climate_change_updated_2_0_Copy1_TEST. version 2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "320c1f05b41b6296d6cdeadbc8f37198b22e160db062b16d8b8cc9d95c25d782"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
